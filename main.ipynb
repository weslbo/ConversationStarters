{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import requests\n",
    "import markdownify\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "\n",
    "Here we define the course code (for example DP-203), and define a collection of learning paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"SC-100\"\n",
    "\n",
    "list_of_learn_paths = [\n",
    "    \"https://learn.microsoft.com/en-us/training/paths/sc-100-design-solutions-best-practices-priorities/\",\n",
    "    \"https://learn.microsoft.com/en-us/training/paths/sc-100-design-operations-identity-compliance-capabilities/\",\n",
    "    \"https://learn.microsoft.com/en-us/training/paths/sc-100-design-security-solutions-applications-data/\",\n",
    "    \"https://learn.microsoft.com/en-us/training/paths/sc-100-design-security-solutions-infrastructure/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve learning path modules\n",
    "\n",
    "The code below is a Python script designed to scrape data from a list of URLs, parse the HTML content, and save the extracted information into a JSON file. The script uses the `requests` library to fetch HTML content from the URLs and `BeautifulSoup` from the `bs4` library to parse the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for url in list_of_learn_paths:\n",
    "    learn_path = requests.get(url)\n",
    "    soup_learnpath = BeautifulSoup(learn_path.text, \"html.parser\")\n",
    "    links_learnpath = soup_learnpath.find_all(class_=\"font-size-h6\")\n",
    "    title_learnpath = soup_learnpath.find(\"h1\", class_=\"title\").text\n",
    "    absolute_urls = [urljoin(url, link[\"href\"]) for link in links_learnpath]\n",
    "\n",
    "    print(title_learnpath)\n",
    "\n",
    "    jsondata_learnpath = {} \n",
    "    jsondata_learnpath[\"learning_path\"] = title_learnpath\n",
    "    jsondata_learnpath[\"url\"] = url \n",
    "    jsondata_learnpath[\"learning_modules\"] = []\n",
    "\n",
    "    for module in absolute_urls:\n",
    "        learn_module = requests.get(module)\n",
    "        soup_learnmodule = BeautifulSoup(learn_module.text, \"html.parser\")\n",
    "        links_units = soup_learnmodule.find_all(class_=\"unit-title\")\n",
    "        links_units = [link for link in links_units if not any(keyword in link[\"href\"] for keyword in [\"exercise\", \"knowledge-check\", \"summary\"])]\n",
    "        title_module = soup_learnmodule.find(\"h1\", class_=\"title\").text\n",
    "        absolute_urls_units = [urljoin(module, link[\"href\"]) for link in links_units]\n",
    "\n",
    "        print(\"- \" + title_module)\n",
    "\n",
    "        jsondata_learnmodule = {} \n",
    "        jsondata_learnmodule[\"learning_module\"] = title_module\n",
    "        jsondata_learnmodule[\"url\"] = module \n",
    "        jsondata_learnmodule[\"learning_units\"] = absolute_urls_units\n",
    "\n",
    "        jsondata_learnpath[\"learning_modules\"].append(jsondata_learnmodule)\n",
    "\n",
    "    data.append(jsondata_learnpath)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(\"./temp\"):\n",
    "    os.makedirs(\"./temp\")\n",
    "    \n",
    "# Write the data to the JSON file\n",
    "with open(\"./temp/LearningPaths.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve content\n",
    "\n",
    "The code below is a Python function named `get_markdown` that retrieves and processes HTML content from a given URL, converting it into `Markdown` format and saving it to a specified location. This function relies on several external libraries, including `requests`, `BeautifulSoup`, and `markdownify`.\n",
    "\n",
    "While scraping the learn unit, a few html tags need to be removed.\n",
    "\n",
    "The code will process learning paths from a JSON file, create corresponding directories, and convert HTML content from specified URLs into Markdown format. The code begins by opening and reading a JSON file named LearningPaths.json located in the ./temp/ directory. This file contains a list of learning paths, each with associated learning modules and units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markdown(url, savelocation):\n",
    "    print(\"- Retrieving markdown from \" + url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # might need to adapt this when working with other web pages (not Microsoft Learn)\n",
    "    div = soup.find(id=\"unit-inner-section\")\n",
    "\n",
    "    for ul in div.find_all(\"ul\", class_=\"metadata\"):\n",
    "        ul.decompose()\n",
    "    for d in div.find_all(\"div\", class_=\"xp-tag\"):\n",
    "        d.decompose()\n",
    "    for next in div.find_all(\"div\", class_=\"next-section\"):\n",
    "        next.decompose()\n",
    "    for header in div.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        header.string = \"\\n# \" + header.get_text() + \"\\n\"\n",
    "    for code in div.find_all(\"code\"):\n",
    "        code.decompose()\n",
    "\n",
    "    markdown = markdownify.markdownify(str(div), heading_style=\"ATX\", bullets=\"-\")\n",
    "    markdown = re.sub('\\n{3,}', '\\n\\n', markdown)\n",
    "    markdown = markdown.replace(\"[Continue](/en-us/)\", \"\")\n",
    "\n",
    "    with open(savelocation, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(markdown)\n",
    "\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./temp/LearningPaths.json\", \"r\") as file:\n",
    "    learning_paths = json.load(file)\n",
    "    \n",
    "if not os.path.exists(f\"./temp/{code}\"):\n",
    "    os.mkdir(f\"./temp/{code}\")\n",
    "\n",
    "iModule = 1\n",
    "for lp in learning_paths:\n",
    "    modules = [module for module in lp[\"learning_modules\"]]\n",
    "    \n",
    "    for module in modules:\n",
    "        outputFolder_module = f\"temp/{code}/{iModule}.{module['learning_module']}\"\n",
    "        iModule += 1\n",
    "        \n",
    "        if not os.path.exists(outputFolder_module):\n",
    "            os.mkdir(outputFolder_module)\n",
    "            \n",
    "        for index, url in enumerate(module[\"learning_units\"]):\n",
    "            unit_name = url.split(\"/\")[-1]\n",
    "            \n",
    "            outputFile_md = f\"{outputFolder_module}/{unit_name}.md\"\n",
    "            markdown = get_markdown(url, outputFile_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine (append) all files\n",
    "\n",
    "Once all the content has been scraped (all learning units of a learn module), we combine this into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_md_files(directory):\n",
    "    # Get a list of all .md files in the current directory\n",
    "    md_files = sorted(glob.glob(os.path.join(directory, \"[!0]*.md\")), key=lambda x: os.path.basename(x))\n",
    "    md_files = [file for file in md_files if not file.endswith('.transcript.md')]\n",
    "    output_file = os.path.join(directory, \"0-Content.md\")\n",
    "    \n",
    "    print(directory)\n",
    "    print(output_file)\n",
    "\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        # Iterate over each .md file\n",
    "        for md_file in md_files:\n",
    "            # Open each .md file in read mode\n",
    "            with open(md_file, 'r') as infile:\n",
    "                # Read the content and write it to the output file\n",
    "                outfile.write(infile.read())\n",
    "                # Optionally, add a newline or some separator between files\n",
    "                outfile.write(\"\\n\\n\")  # Adds a newline for separation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iModule = 1\n",
    "for lp in learning_paths:\n",
    "    modules = [module for module in lp[\"learning_modules\"]]\n",
    "    \n",
    "    for module in modules:\n",
    "        outputFolder_module = f\"temp/{code}/{iModule}.{module['learning_module']}\"\n",
    "        iModule += 1\n",
    "        \n",
    "        append_md_files(outputFolder_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate conversation starters\n",
    "\n",
    "First, we create an instance op Azure Open AI. (Make sure to have the correct environment variables in a .env file)\n",
    "\n",
    "Then, we pass in a prompt (see below) plus the content from the learn module and store the output as a markdown file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"), api_version=\"2024-02-15-preview\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConversationStarters(contentLocation, savelocation):\n",
    "    mdFile = contentLocation + \"/0-Content.md\"\n",
    "    print(\"- Generating conversation starters for \" + mdFile)\n",
    "    \n",
    "    with open(mdFile, \"r\") as file:\n",
    "        contents = file.read()\n",
    "    \n",
    "    prompt = \"\"\" \n",
    "We're going to learn a new topic today. \n",
    "\n",
    "Create a list of 5 open ended questions on this content that help learners become interested in the course.\n",
    "\n",
    "Create a mindmap of all topics covered. Use a hierachical structure as an ASCII tree diagram\n",
    "\n",
    "Create a practice assesment of up to 10 questions. It's OK to have challenging questions, do not make it too obvious. Provide the answer as well. Make sure to generate a mix of multiple choice, true/false (provide the choices), fill-in-the-blanks questions (provide suggestions to choose from).\n",
    "\n",
    "Create one complex problem related to the training content. I will use this to encourage group discussions to solve the problem. \n",
    "\n",
    "============\n",
    "{contents}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = prompt.replace(\"{contents}\", contents)\n",
    "    \n",
    "    message_text = [\n",
    "        {\"role\":\"system\",\"content\":prompt},\n",
    "        {\"role\":\"user\",\"content\":\"Generate the questions, mindmap and practice assesment. Generate also the complex problem\"}\n",
    "    ]\n",
    "        \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages = message_text,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )    \n",
    "    \n",
    "    output = completion.choices[0].message.content\n",
    "    print(f\"- Actual total usage token={completion.usage.total_tokens}\")\n",
    "    \n",
    "    with open(savelocation, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./output\"):\n",
    "    os.mkdir(f\"./output\")\n",
    "    \n",
    "if not os.path.exists(f\"./output/{code}\"):\n",
    "    os.mkdir(f\"./output/{code}\")\n",
    "    \n",
    "iModule = 1  \n",
    "for lp in learning_paths:\n",
    "    modules = [module for module in lp[\"learning_modules\"]]\n",
    "\n",
    "    for module in modules:\n",
    "        contentFolder_module = f\"temp/{code}/{iModule}.{module['learning_module']}\"\n",
    "        saveLocation = f\"output/{code}/{iModule}.{module['learning_module']}.md\"\n",
    "        saveLocation = saveLocation.replace(\":\", \" -\")\n",
    "        \n",
    "        iModule += 1\n",
    "        \n",
    "        generateConversationStarters(contentFolder_module, saveLocation)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./temp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
