# Open-ended Questions

1. How does Delta Lake enhance Spark-based data lake processing?
2. What are the benefits of using Delta tables in a Microsoft Fabric lakehouse?
3. How can you create a delta table from a dataframe in Spark?
4. What is the difference between managed and external tables in Delta Lake?
5. How can Delta Lake be used with streaming data in Spark?

# Mindmap

```
Delta Lake in Microsoft Fabric
├── Introduction
│   ├── Delta Lake Overview
│   └── Benefits
├── Understand Delta Lake
│   ├── Relational Tables
│   ├── ACID Transactions
│   ├── Data Versioning
│   ├── Batch and Streaming Support
│   └── Standard Formats
├── Create Delta Tables
│   ├── From Dataframe
│   ├── Managed vs External Tables
│   ├── Table Metadata
│   │   ├── DeltaTableBuilder API
│   │   └── Spark SQL
│   └── Saving Data in Delta Format
├── Work with Delta Tables in Spark
│   ├── Using Spark SQL
│   ├── Delta API
│   └── Time Travel
└── Use Delta Tables with Streaming Data
    ├── Spark Structured Streaming
    ├── Streaming Source
    └── Streaming Sink
```

# Practice Assessment

1. **Multiple Choice:**  
   What format are Delta Lake data files stored in?  
   a) CSV  
   b) JSON  
   c) Parquet  
   d) XML  
   **Answer:** c) Parquet

2. **True/False:**  
   Delta Lake supports ACID transactions.  
   a) True  
   b) False  
   **Answer:** a) True

3. **Fill-in-the-Blanks:**  
   Delta Lake tables can be used as both _______ and _______ for streaming data.  
   (Suggestions: sources, sinks, tables, logs)  
   **Answer:** sources, sinks

4. **Multiple Choice:**  
   Which API can be used to create a table with specific columns in Delta Lake?  
   a) DeltaTableBuilder API  
   b) DeltaStream API  
   c) DeltaFormat API  
   d) DeltaSchema API  
   **Answer:** a) DeltaTableBuilder API

5. **True/False:**  
   Deleting an external table from the lakehouse metastore deletes the associated data files.  
   a) True  
   b) False  
   **Answer:** b) False

6. **Fill-in-the-Blanks:**  
   The _______ folder contains transaction logs for a Delta table.  
   (Suggestions: _delta_log, _transaction_log, _data_log, _spark_log)  
   **Answer:** _delta_log

7. **Multiple Choice:**  
   What feature allows you to retrieve a previous version of a row in a Delta table?  
   a) Data Recovery  
   b) Time Travel  
   c) Version Control  
   d) Snapshot Retrieval  
   **Answer:** b) Time Travel

8. **True/False:**  
   Delta Lake can only be used with batch data, not streaming data.  
   a) True  
   b) False  
   **Answer:** b) False

9. **Fill-in-the-Blanks:**  
   In Spark, you can use _______ to embed SQL statements in other languages.  
   (Suggestions: spark.sql, spark.query, spark.execute, spark.run)  
   **Answer:** spark.sql

10. **Multiple Choice:**  
    What is the purpose of a checkpoint file in streaming with Delta Lake?  
    a) To store data permanently  
    b) To track the state of stream processing  
    c) To delete old data  
    d) To enhance data security  
    **Answer:** b) To track the state of stream processing

# Complex Problem

**Problem:**  
Imagine you are tasked with designing a data processing pipeline for a retail company using Delta Lake in Microsoft Fabric. The company wants to capture real-time sales data from multiple stores and analyze it for trends and inventory management. The data should be stored in a way that allows for both historical analysis and real-time reporting.

**Discussion Points:**

- How would you set up the data ingestion process using Delta Lake?
- What considerations would you make for using managed vs. external tables?
- How would you implement time travel to analyze historical sales data?
- How would you handle streaming data to ensure real-time reporting?
- What challenges might you face in maintaining data consistency and integrity?

Encourage the group to discuss these points and come up with a comprehensive solution.