# Open-Ended Questions

1. How do Fabric notebooks enhance the ETL process compared to other ingestion options?
2. What are the benefits of using Parquet files in a lakehouse?
3. How can you optimize Delta table writes in Fabric notebooks?
4. What considerations should be made when transforming data for different users?
5. How does the use of Spark in Fabric notebooks facilitate data ingestion and transformation?

# Mindmap

```
Microsoft Fabric for Analytics
├── Ingest Data
│   ├── ETL Process
│   ├── Fabric Notebooks
│   │   ├── Automation
│   │   ├── Code Cells
│   │   ├── PySpark
│   ├── Connect to Data
│   │   ├── External Sources
│   │   ├── Authentication
│   └── Write Data
│       ├── File Formats
│       ├── Delta Tables
│       └── Optimization
│           ├── V-Order
│           └── Optimize Write
└── Data Transformation
    ├── User Needs
    ├── Data Cleaning
    ├── Data Wrangler
    └── Power BI Integration
```

# Practice Assessment

1. **Multiple Choice:**  
   What is the primary advantage of using Fabric notebooks for data ingestion?  
   a) Manual uploads  
   b) Automation and systematic approach  
   c) UI experience  
   d) Limited to small datasets  
   **Answer:** b) Automation and systematic approach

2. **True/False:**  
   Fabric notebooks can only use PySpark for data processing.  
   a) True  
   b) False  
   **Answer:** b) False

3. **Fill-in-the-Blanks:**  
   Delta tables in a lakehouse are based on the _______ format.  
   (Suggestions: JSON, Parquet, CSV)  
   **Answer:** Parquet

4. **Multiple Choice:**  
   Which optimization technique applies special sorting and compression on parquet files?  
   a) Data Wrangling  
   b) V-Order  
   c) Data Cleansing  
   d) Manual Sorting  
   **Answer:** b) V-Order

5. **True/False:**  
   Data scientists prefer heavily transformed data for their analysis.  
   a) True  
   b) False  
   **Answer:** b) False

6. **Fill-in-the-Blanks:**  
   _______ tables allow for efficient data ingestion and loading in Fabric lakehouses.  
   (Suggestions: Delta, JSON, CSV)  
   **Answer:** Delta

7. **Multiple Choice:**  
   What should be considered when transforming data for Power BI analysts?  
   a) Raw data access  
   b) Minimal transformation  
   c) Well-prepared data  
   d) Ignoring data quality  
   **Answer:** c) Well-prepared data

8. **True/False:**  
   Parquet files are preferred due to their row-based storage structure.  
   a) True  
   b) False  
   **Answer:** b) False

9. **Fill-in-the-Blanks:**  
   The _______ layer of a Medallion architecture is where raw data is ingested.  
   (Suggestions: Silver, Gold, Bronze)  
   **Answer:** Bronze

10. **Multiple Choice:**  
    Which of the following is not a supported file format in OneLake?  
    a) Avro  
    b) ORC  
    c) XML  
    d) Parquet  
    **Answer:** c) XML

# Complex Problem

**Scenario:**  
Your team is tasked with setting up a data pipeline using Microsoft Fabric to ingest, transform, and analyze sales data from multiple external sources. The data needs to be ingested into a lakehouse, transformed for different user needs, and optimized for reporting in Power BI.

**Problem:**  
Design a comprehensive ETL strategy using Fabric notebooks that addresses the following:

- Efficiently ingest large datasets from Azure Blob Storage and an Azure SQL Database.
- Implement data transformations to clean and prepare the data for both data scientists and Power BI analysts.
- Optimize the data storage and retrieval processes using Delta tables and appropriate file formats.
- Ensure the solution is scalable and can handle increasing data volumes over time.

**Discussion Points:**

- What are the key steps in setting up the data pipeline?
- How would you handle authentication for different data sources?
- What transformations are necessary for different user groups?
- How can you ensure the solution remains efficient as data volumes grow?