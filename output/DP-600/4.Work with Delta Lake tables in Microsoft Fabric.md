# Open-ended Questions

1. How does Delta Lake enhance Spark-based data lake processing?
2. What are the benefits of using Delta tables in a Microsoft Fabric lakehouse?
3. How can you create a delta table from a dataframe in Spark?
4. What is the difference between managed and external delta tables?
5. How can you use Delta Lake's time travel feature to work with table versioning?

# Mindmap

```
Delta Lake in Microsoft Fabric
├── Introduction
│   ├── Delta Lake Overview
│   ├── Lakehouse Architecture
├── Understand Delta Lake
│   ├── Delta Tables
│   │   ├── Schema Abstractions
│   │   ├── Parquet Files
│   │   ├── Transaction Logs
│   ├── Benefits
│   │   ├── Relational Tables
│   │   ├── ACID Transactions
│   │   ├── Data Versioning
│   │   ├── Batch and Streaming Support
│   │   ├── Standard Formats
├── Create Delta Tables
│   ├── Dataframe to Delta Table
│   ├── Managed vs External Tables
│   ├── Creating Table Metadata
│   │   ├── DeltaTableBuilder API
│   │   ├── Spark SQL
│   ├── Saving Data in Delta Format
├── Work with Delta Tables in Spark
│   ├── Using Spark SQL
│   ├── Delta API
│   ├── Time Travel
├── Use Delta Tables with Streaming Data
│   ├── Spark Structured Streaming
│   ├── Streaming with Delta Tables
│   │   ├── Streaming Source
│   │   ├── Streaming Sink
```

# Practice Assessment

1. **Multiple Choice:**  
   What format are the underlying data files for Delta tables stored in?  
   a) JSON  
   b) CSV  
   c) Parquet  
   d) XML  
   **Answer:** c) Parquet

2. **True/False:**  
   Delta Lake supports ACID transactions.  
   a) True  
   b) False  
   **Answer:** a) True

3. **Fill in the Blanks:**  
   Delta Lake enables data versioning and ________, allowing retrieval of previous data versions.  
   (Options: time travel, data replication, data encryption)  
   **Answer:** time travel

4. **Multiple Choice:**  
   Which API can be used to create a table based on specific specifications in Delta Lake?  
   a) DeltaTableBuilder API  
   b) DeltaStream API  
   c) DeltaSchema API  
   d) DeltaData API  
   **Answer:** a) DeltaTableBuilder API

5. **True/False:**  
   Deleting an external table from the lakehouse metastore also deletes the associated data files.  
   a) True  
   b) False  
   **Answer:** b) False

6. **Fill in the Blanks:**  
   A Spark Structured Streaming dataframe can read data from ________ sources.  
   (Options: static, streaming, batch)  
   **Answer:** streaming

7. **Multiple Choice:**  
   What operation is not allowed when using a delta table as a streaming source?  
   a) Append  
   b) Update  
   c) Insert  
   d) Select  
   **Answer:** b) Update

8. **True/False:**  
   Delta Lake tables can be used as both sinks and sources for streaming data.  
   a) True  
   b) False  
   **Answer:** a) True

9. **Fill in the Blanks:**  
   The ________ folder contains transaction log files for a delta table.  
   (Options: _delta_log, _transaction_log, _data_log)  
   **Answer:** _delta_log

10. **Multiple Choice:**  
    Which of the following is a benefit of using Delta tables?  
    a) Lack of support for SQL queries  
    b) No data versioning  
    c) Support for batch and streaming data  
    d) Incompatibility with Parquet format  
    **Answer:** c) Support for batch and streaming data

# Complex Problem

**Problem:**  
Imagine you are tasked with designing a data processing pipeline for a company that collects real-time sensor data from IoT devices. The data needs to be stored in a way that allows for both real-time analytics and historical analysis. Discuss how you would use Delta Lake in Microsoft Fabric to achieve this. Consider aspects such as data ingestion, storage, querying, and handling of both batch and streaming data. What challenges might you face, and how would you address them?

**Discussion Points:**
- How to set up the data ingestion pipeline using Spark Structured Streaming.
- The role of Delta tables as both sinks and sources.
- How to implement time travel for historical analysis.
- Strategies for ensuring data consistency and reliability.
- Potential challenges with data volume and velocity, and solutions to mitigate them.