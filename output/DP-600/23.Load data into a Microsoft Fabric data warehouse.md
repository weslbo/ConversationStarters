# Open-Ended Questions

1. How does Microsoft Fabric Data Warehouse utilize Synapse Analytics to enhance data management and analysis?
2. What are the key differences between data ingestion and data loading in the ETL process?
3. How do staging areas benefit the data loading process in a data warehouse?
4. What are the advantages and challenges of using incremental loads in a data warehouse?
5. How can Copilot assist in transforming data within Dataflow Gen2?

# Mindmap

```
Microsoft Fabric Data Warehouse
├── Introduction
│   ├── Synapse Analytics
│   ├── OneLake
│   └── Parquet File Format
├── ETL Process
│   ├── Data Extraction
│   ├── Data Transformation
│   ├── Data Loading
│   └── Post-load Optimizations
├── Data Load Strategies
│   ├── Data Ingestion vs. Data Loading
│   ├── Staging
│   ├── Full Load
│   └── Incremental Load
├── Dimension and Fact Tables
│   ├── Dimension Table
│   ├── Fact Table
│   └── Slowly Changing Dimensions (SCD)
├── Data Pipelines
│   ├── Creating Pipelines
│   ├── Copy Data Assistant
│   ├── Scheduling
│   └── Integration with Azure Data Factory
├── T-SQL Data Loading
│   ├── COPY Statement
│   ├── Error Handling
│   ├── Loading Multiple Files
│   └── Cross-database Queries
└── Dataflow Gen2
    ├── Creating Dataflows
    ├── Importing Data
    ├── Transforming Data with Copilot
    ├── Adding Data Destinations
    └── Publishing Dataflows
```

# Practice Assessment

1. **Multiple Choice:**  
   What file format is used to store data in Microsoft Fabric's data warehouse?
│   ├── Slowly Changing Dimensions (SCD)
│   └── Types of SCD
├── Data Pipelines
│   ├── Creating Pipelines
│   ├── Copy Data Assistant
│   ├── Scheduling
│   └── Integration with Azure Data Factory
├── Loading Data with T-SQL
│   ├── COPY Statement
│   ├── Error Handling
│   ├── Loading Multiple Files
│   └── Cross-Database Queries
└── Dataflow Gen2
    ├── Creating Dataflows
    ├
├── Load Data Using T-SQL
│   ├── COPY Statement
│   ├── Error Handling
│   ├── Load Multiple Files
│   └── Cross-database Queries
└── Dataflow Gen2
    ├── Creating Dataflows
    ├── Importing Data
    ├── Transforming Data with Copilot
    ├── Adding Data Destination
    └── Publishing Dataflows
```

# Practice Assessment

1. **Multiple Choice:**  
   What is the primary file format used for storing data in Microsoft Fabric Data Warehouse?  
   a) CSV  
   b) JSON  
   c) Parquet  
   d) XML  
   **Answer:** c) Parquet

2. **True/False:**  
   The ETL process must always be completed sequentially.  
   a) True  
   b) False  
   **Answer:** b) False

3. **Fill-in-the-Blanks:**  
   In a data warehouse, a dimension table provides the _______ to the raw numbers found in the fact tables.  
   Options: context, data, analysis, storage  
   **Answer:** context

4. **Multiple Choice:**  
   Which type of Slowly Changing Dimension (SCD) keeps full history for a given natural key?  
   a) Type 1  
   b) Type 2  
   c) Type 3  
   d) Type 4  
   **Answer:** b) Type 2

5. **True/False:**  
   A full load in a data warehouse preserves historical data.  
   a) True  
   b) False  
   **Answer:** b) False

6. **Fill-in-the-Blanks:**  
   The _______ statement is used for importing data into the Warehouse from an external Azure storage account.  
   Options: INSERT, COPY, SELECT, UPDATE  
   **Answer:** COPY

7. **Multiple Choice:**  
   What is the main advantage of using a staging area in data loading?  
   a) Increases data redundancy  
   b) Simplifies and facilitates the load operation  
   c) Reduces data quality  
   d) Increases data loading time  
   **Answer:** b) Simplifies and facilitates the load operation

8. **True/False:**  
   Data pipelines in Microsoft Fabric are based on Azure Data Factory.  
   a) True  
   b) False  
   **Answer:** a) True

9. **Fill-in-the-Blanks:**  
   In Dataflow Gen2, the _______ feature allows for the separation of ETL logic and destination storage.  
   Options: Add data destination, Transform data, Schedule, Publish  
   **Answer:** Add data destination

10. **Multiple Choice:**  
    Which of the following is NOT a destination option for Dataflow Gen2?  
    a) Azure SQL Database  
    b) Lakehouse  
    c) Google Cloud Storage  
    d) Azure Synapse Analytics  
    **Answer:** c) Google Cloud Storage

# Complex Problem

**Problem:**  
Your team is tasked with setting up a data warehouse for an online retail company using Microsoft Fabric. The company wants to track sales data, customer information, and product details. They need to ensure that historical data is preserved for customer addresses, and they want to update the warehouse with new sales data every hour. Additionally, they want to use Dataflow Gen2 to transform and load data into the warehouse.

**Discussion Points:**

1. How would you design the ETL process to handle the initial full load and subsequent incremental loads?
2. What type of Slowly Changing Dimension (SCD) would you use for customer addresses, and why?
3. How would you utilize staging areas to optimize the data loading process?
4. What data pipeline strategies would you implement to ensure timely and accurate data updates?
5. How can Copilot be used in Dataflow Gen2 to assist with data transformation tasks?

Encourage the group to discuss these points and come up with a comprehensive plan for setting up the data warehouse.