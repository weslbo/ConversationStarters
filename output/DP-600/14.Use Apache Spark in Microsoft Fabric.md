# Open-Ended Questions

1. How does Apache Spark's "divide and conquer" approach work in a Spark pool?
2. What are the benefits of using custom environments in Microsoft Fabric for Spark processing?
3. How can the native execution engine improve performance in Microsoft Fabric?
4. What is the difference between a Spark notebook and a Spark job definition?
5. How can partitioning data improve performance in Spark?

# Mindmap

```
Apache Spark in Microsoft Fabric
├── Introduction
│   ├── Overview
│   └── Platforms
├── Prepare to use Apache Spark
│   ├── Spark Pools
│   │   ├── Head Node
│   │   └── Worker Nodes
│   ├── Spark Pools in Microsoft Fabric
│   │   ├── Starter Pool
│   │   └── Custom Pools
│   ├── Runtimes and Environments
│   │   ├── Spark Runtimes
│   │   └── Custom Environments
│   ├── Additional Configuration
│   │   ├── Native Execution Engine
│   │   ├── High Concurrency Mode
│   │   └── MLFlow Logging
│   └── Administration
├── Run Spark Code
│   ├── Notebooks
│   └── Spark Job Definition
├── Work with Data in a Spark Dataframe
│   ├── Dataframe vs RDD
│   ├── Loading Data
│   │   ├── Inferring Schema
│   │   └── Explicit Schema
│   ├── Data Manipulation
│   │   ├── Filtering
│   │   ├── Grouping
│   │   └── Saving Data
│   ├── Partitioning
│   └── Loading Partitioned Data
├── Work with Data using Spark SQL
│   ├── Spark Catalog
│   │   ├── Temporary Views
│   │   └── Tables
│   ├── SQL API
│   └── SQL Code
└── Visualize Data in a Spark Notebook
    ├── Built-in Charts
    └── Graphics Packages
```

# Practice Assessment

1. **Multiple Choice:**  
   What is the primary benefit of using a Spark pool in Microsoft Fabric?  
   a) To store data  
   b) To distribute data processing tasks  
   c) To visualize data  
   d) To manage user access  
   **Answer:** b) To distribute data processing tasks

2. **True/False:**  
   Custom environments in Microsoft Fabric can only use the default Spark runtime.  
   a) True  
   b) False  
   **Answer:** b) False

3. **Fill-in-the-Blanks:**  
   The native execution engine in Microsoft Fabric is a __________ processing engine that runs Spark operations directly on lakehouse infrastructure.  
   (Options: vectorized, serialized, parallel)  
   **Answer:** vectorized

4. **Multiple Choice:**  
   Which of the following is NOT a method to manipulate data in a Spark dataframe?  
   a) select  
   b) groupBy  
   c) visualize  
   d) filter  
   **Answer:** c) visualize

5. **True/False:**  
   Partitioning data in Spark can lead to performance improvements.  
   a) True  
   b) False  
   **Answer:** a) True

6. **Fill-in-the-Blanks:**  
   A Spark __________ is used to explore and analyze data interactively in Microsoft Fabric.  
   (Options: notebook, job, pool)  
   **Answer:** notebook

7. **Multiple Choice:**  
   What is the purpose of MLFlow in Microsoft Fabric?  
   a) To log machine learning experiment activity  
   b) To visualize data  
   c) To manage Spark pools  
   d) To create custom environments  
   **Answer:** a) To log machine learning experiment activity

8. **True/False:**  
   A temporary view in Spark is automatically deleted at the end of the current session.  
   a) True  
   b) False  
   **Answer:** a) True

9. **Fill-in-the-Blanks:**  
   In a Spark notebook, you can use the __________ magic to run SQL code.  
   (Options: %sql, %pyspark, %scala)  
   **Answer:** %sql

10. **Multiple Choice:**  
    Which Python library is commonly used for creating data visualizations in Spark notebooks?  
    a) NumPy  
    b) Matplotlib  
    c) TensorFlow  
    d) Pandas  
    **Answer:** b) Matplotlib

# Complex Problem

**Problem:**  
You are tasked with designing a data processing pipeline using Apache Spark in Microsoft Fabric for a retail company. The company wants to analyze sales data to identify trends and optimize inventory. The data is stored in CSV files in a lakehouse and includes sales transactions, product details, and customer information.

**Tasks:**

1. Design a Spark pool configuration that balances performance and cost.
2. Create a custom environment with necessary libraries for data analysis.
3. Develop a Spark job to ingest and transform the sales data into a structured format.
4. Use Spark SQL to create a temporary view for querying sales trends.
5. Visualize the sales trends using a Python graphics library.

**Discussion Points:**

- How would you configure the Spark pool to handle varying data volumes efficiently?
- What libraries would you include in the custom environment for data analysis?
- How would you ensure data quality and consistency during the transformation process?
- What SQL queries would you use to identify sales trends?
- How would you present the visualized data to stakeholders?