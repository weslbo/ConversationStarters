# Open-ended Questions

1. How does Apache Spark's "divide and conquer" approach work in a Spark pool?
2. What are the benefits of using PySpark and Spark SQL for data engineering tasks?
3. How can you customize Spark pools in Microsoft Fabric to optimize performance?
4. What is the role of the Spark catalog in managing relational data objects?
5. How can you visualize data in a Spark notebook using Python libraries?

# Mindmap

```
Apache Spark in Microsoft Fabric
├── Introduction
│   ├── Overview
│   └── Integration with Microsoft Fabric
├── Prepare to use Apache Spark
│   ├── Spark Pools
│   │   ├── Head Node
│   │   └── Worker Nodes
│   ├── Spark Pools in Microsoft Fabric
│   │   ├── Starter Pool
│   │   └── Custom Spark Pools
│   ├── Runtimes and Environments
│   │   ├── Spark Runtimes
│   │   └── Custom Environments
│   ├── Additional Spark Configuration
│   │   ├── Native Execution Engine
│   │   ├── High Concurrency Mode
│   │   └── Automatic MLFlow Logging
│   └── Spark Administration
├── Run Spark Code
│   ├── Notebooks
│   └── Spark Job Definition
├── Work with Data in a Spark Dataframe
│   ├── Loading Data
│   │   ├── Inferring Schema
│   │   └── Specifying Schema
│   ├── Filtering and Grouping
│   ├── Saving Dataframe
│   │   ├── Parquet Format
│   │   └── Partitioning
│   └── Load Partitioned Data
├── Work with Data using Spark SQL
│   ├── Spark Catalog
│   │   ├── Temporary Views
│   │   └── Tables
│   ├── Spark SQL API
│   └── SQL Code in Notebooks
└── Visualize Data in a Spark Notebook
    ├── Built-in Notebook Charts
    └── Graphics Packages
```

# Practice Assessment

1. **Multiple Choice:**  
   What is the primary benefit of using a Spark pool in Microsoft Fabric?  
   a) To store large datasets  
   b) To coordinate distributed data processing  
   c) To visualize data  
   d) To manage user access  

   **Answer:** b) To coordinate distributed data processing

2. **True/False:**  
   PySpark is a variant of Python specifically designed for Spark.  
   a) True  
   b) False  

   **Answer:** a) True

3. **Fill-in-the-Blanks:**  
   The Spark catalog is a metastore for relational data objects such as ______ and ______.  
   (Suggestions: views, tables, nodes, clusters)  

   **Answer:** views, tables

4. **Multiple Choice:**  
   Which of the following is NOT a configuration setting for Spark pools in Microsoft Fabric?  
   a) Node Family  
   b) Autoscale  
   c) Data Visualization  
   d) Dynamic Allocation  

   **Answer:** c) Data Visualization

5. **True/False:**  
   The native execution engine in Microsoft Fabric improves performance by using vectorized processing.  
   a) True  
   b) False  

   **Answer:** a) True

6. **Fill-in-the-Blanks:**  
   In Spark, a dataframe is similar to those in the ______ Python library but optimized for distributed processing.  
   (Suggestions: Pandas, NumPy, Matplotlib, Seaborn)  

   **Answer:** Pandas

7. **Multiple Choice:**  
   What is the purpose of partitioning data when saving a dataframe?  
   a) To reduce storage space  
   b) To improve query performance  
   c) To enhance data security  
   d) To simplify data visualization  

   **Answer:** b) To improve query performance

8. **True/False:**  
   High concurrency mode allows multiple users to share Spark sessions without isolation.  
   a) True  
   b) False  

   **Answer:** b) False

9. **Fill-in-the-Blanks:**  
   The ______ library is commonly used in Python to create data visualizations in Spark notebooks.  
   (Suggestions: Matplotlib, PySpark, Spark SQL, Delta Lake)  

   **Answer:** Matplotlib

10. **Multiple Choice:**  
    Which method is used to load partitioned data into a Spark dataframe?  
    a) loadPartition  
    b) readPartition  
    c) partitionBy  
    d) read  

    **Answer:** d) read

# Complex Problem

**Problem:**  
You are tasked with designing a data processing pipeline using Apache Spark in Microsoft Fabric for a retail company. The company wants to analyze sales data to identify trends and optimize inventory. The data is stored in CSV format in a lakehouse and includes columns for ProductID, ProductName, Category, SaleDate, and SaleAmount.

**Tasks:**

1. Design a Spark pool configuration that optimizes performance and cost.
2. Create a Spark job to load the CSV data into a dataframe, ensuring the schema is correctly defined.
3. Use Spark SQL to create a temporary view and write a query to calculate total sales per category.
4. Visualize the results using a Python graphics library in a notebook.
5. Discuss how you would partition the data for future analysis and why this is beneficial.

**Discussion Points:**

- Considerations for node configuration and autoscaling in the Spark pool.
- The importance of schema definition and data types in the dataframe.
- How SQL queries can be optimized for performance.
- The choice of visualization library and customization options.
- The impact of partitioning on query performance and storage efficiency.